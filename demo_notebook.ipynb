{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ±ðŸ¶ Cat vs Dog Classification - Interactive Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow for training and evaluating a cat vs dog classifier with advanced features.\n",
    "\n",
    "## Features:\n",
    "- Custom CNN with attention mechanisms\n",
    "- Advanced data augmentation\n",
    "- Grad-CAM visualization\n",
    "- Model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check GPU\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Import project modules\n",
    "sys.path.append('..')\n",
    "from models.architectures import build_custom_cnn, build_efficientnet_transfer\n",
    "from utils.augmentation import AdvancedAugmentation, MixupCutmix\n",
    "from utils.gradcam import GradCAM\n",
    "from utils.training import TrainingConfig, TrainingPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and visualize the custom CNN architecture\n",
    "model = build_custom_cnn(input_shape=(224, 224, 3))\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.reduce_prod(v.shape) for v in model.trainable_variables]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    dpi=96\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Augmentation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image\n",
    "sample_image_path = '../data/cat.0.jpg'  # Adjust path as needed\n",
    "\n",
    "if os.path.exists(sample_image_path):\n",
    "    sample_image = cv2.imread(sample_image_path)\n",
    "    sample_image = cv2.cvtColor(sample_image, cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    # Create a dummy image if no real data available\n",
    "    sample_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "    print(\"Using dummy image for demonstration\")\n",
    "\n",
    "# Test different augmentation levels\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Data Augmentation Examples', fontsize=16, fontweight='bold')\n",
    "\n",
    "augmentation_levels = ['light', 'medium', 'heavy']\n",
    "\n",
    "for row_idx, level in enumerate(augmentation_levels):\n",
    "    aug = AdvancedAugmentation(img_size=224, augmentation_level=level)\n",
    "    \n",
    "    for col_idx in range(4):\n",
    "        if col_idx == 0 and row_idx == 0:\n",
    "            # Show original in first cell\n",
    "            axes[row_idx, col_idx].imshow(cv2.resize(sample_image, (224, 224)))\n",
    "            axes[row_idx, col_idx].set_title('Original', fontweight='bold')\n",
    "        else:\n",
    "            # Show augmented versions\n",
    "            augmented = aug(sample_image.copy())\n",
    "            axes[row_idx, col_idx].imshow(augmented)\n",
    "            if col_idx == 0:\n",
    "                axes[row_idx, col_idx].set_title(f'{level.capitalize()} Aug', fontweight='bold')\n",
    "        \n",
    "        axes[row_idx, col_idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mixup and CutMix Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample batch\n",
    "batch_images = np.random.random((4, 224, 224, 3)).astype(np.float32)\n",
    "batch_labels = np.array([0, 1, 0, 1], dtype=np.float32)\n",
    "\n",
    "# Apply Mixup\n",
    "mixup = MixupCutmix(alpha=0.2, cutmix_prob=0.0)\n",
    "mixed_images, mixed_labels = mixup(batch_images.copy(), batch_labels.copy())\n",
    "\n",
    "# Apply CutMix\n",
    "cutmix = MixupCutmix(alpha=0.2, cutmix_prob=1.0)\n",
    "cut_images, cut_labels = cutmix(batch_images.copy(), batch_labels.copy())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "fig.suptitle('Mixup and CutMix Augmentation', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i in range(4):\n",
    "    # Original\n",
    "    axes[0, i].imshow(batch_images[i])\n",
    "    axes[0, i].set_title(f'Original (Label: {batch_labels[i]:.1f})')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Mixup\n",
    "    axes[1, i].imshow(mixed_images[i])\n",
    "    axes[1, i].set_title(f'Mixup (Label: {mixed_labels[i]:.2f})')\n",
    "    axes[1, i].axis('off')\n",
    "    \n",
    "    # CutMix\n",
    "    axes[2, i].imshow(cut_images[i])\n",
    "    axes[2, i].set_title(f'CutMix (Label: {cut_labels[i]:.2f})')\n",
    "    axes[2, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training configuration\n",
    "config = TrainingConfig()\n",
    "\n",
    "# Modify for quick demo (reduce epochs)\n",
    "config.epochs = 10  # Use 50+ for real training\n",
    "config.batch_size = 32\n",
    "config.augmentation_level = 'medium'\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in config.to_dict().items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Schedule Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training import CosineDecayWithWarmup\n",
    "\n",
    "# Create learning rate schedule\n",
    "total_steps = 1000\n",
    "warmup_steps = 100\n",
    "\n",
    "lr_schedule = CosineDecayWithWarmup(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=total_steps - warmup_steps,\n",
    "    warmup_steps=warmup_steps,\n",
    "    alpha=0.01\n",
    ")\n",
    "\n",
    "# Generate learning rates\n",
    "steps = np.arange(0, total_steps)\n",
    "learning_rates = [lr_schedule(step).numpy() for step in steps]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, learning_rates, linewidth=2)\n",
    "plt.axvline(x=warmup_steps, color='r', linestyle='--', label='Warmup End', alpha=0.7)\n",
    "plt.xlabel('Training Step', fontsize=12)\n",
    "plt.ylabel('Learning Rate', fontsize=12)\n",
    "plt.title('Cosine Decay Learning Rate Schedule with Warmup', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Training (Demo)\n",
    "\n",
    "**Note**: This section requires actual cat and dog images. For demonstration purposes, we'll show the training setup. For full training, use `main_train.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example training code (requires real data)\n",
    "# Uncomment and modify paths as needed\n",
    "\n",
    "# from main_train import prepare_data, train_model\n",
    "# \n",
    "# # Prepare data\n",
    "# train_paths, train_labels, val_paths, val_labels, test_paths, test_labels = \\\n",
    "#     prepare_data('../data', img_size=224)\n",
    "# \n",
    "# # Train model\n",
    "# model, history, test_results = train_model(\n",
    "#     data_dir='../data',\n",
    "#     model_type='custom',\n",
    "#     config=config,\n",
    "#     output_dir='../outputs'\n",
    "# )\n",
    "\n",
    "print(\"Training code ready. Use main_train.py for full training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Grad-CAM Visualization Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model (or use the one just trained)\n",
    "# model_path = '../outputs/best_custom_model.keras'\n",
    "# model = keras.models.load_model(model_path)\n",
    "\n",
    "# For demo, we'll use the untrained model\n",
    "model = build_custom_cnn(input_shape=(224, 224, 3))\n",
    "\n",
    "# Create Grad-CAM object\n",
    "grad_cam = GradCAM(model)\n",
    "\n",
    "# Load sample image\n",
    "if os.path.exists(sample_image_path):\n",
    "    test_image = cv2.imread(sample_image_path)\n",
    "    test_image = cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB)\n",
    "else:\n",
    "    test_image = np.random.randint(0, 255, (256, 256, 3), dtype=np.uint8)\n",
    "\n",
    "# Preprocess\n",
    "test_image_resized = cv2.resize(test_image, (224, 224))\n",
    "test_image_processed = test_image_resized.astype(np.float32) / 255.0\n",
    "\n",
    "# Generate Grad-CAM visualization\n",
    "fig = grad_cam.visualize(\n",
    "    image=test_image_processed,\n",
    "    original_image=test_image_resized,\n",
    "    save_path='gradcam_demo.png'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Inference Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path):\n",
    "    \"\"\"Make prediction on a single image\"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    original = img.copy()\n",
    "    \n",
    "    # Preprocess\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Predict\n",
    "    prediction = model.predict(np.expand_dims(img, axis=0), verbose=0)[0][0]\n",
    "    \n",
    "    # Interpret\n",
    "    if prediction > 0.5:\n",
    "        pred_class = \"Dog\"\n",
    "        confidence = prediction\n",
    "    else:\n",
    "        pred_class = \"Cat\"\n",
    "        confidence = 1 - prediction\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cv2.resize(original, (224, 224)))\n",
    "    plt.title(f'Prediction: {pred_class} (Confidence: {confidence:.2%})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return pred_class, confidence\n",
    "\n",
    "# Example usage (requires trained model)\n",
    "# pred_class, confidence = predict_image(model, sample_image_path)\n",
    "print(\"Inference function ready. Use with trained model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes=['Cat', 'Dog']):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('True Label', fontsize=12)\n",
    "    plt.xlabel('Predicted Label', fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "# Example (requires predictions)\n",
    "# y_true = [0, 0, 1, 1, 0, 1]\n",
    "# y_pred = [0, 0, 1, 0, 0, 1]\n",
    "# plot_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion matrix function ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. âœ… Custom CNN architecture with attention mechanisms\n",
    "2. âœ… Advanced data augmentation techniques\n",
    "3. âœ… Mixup and CutMix implementations\n",
    "4. âœ… Learning rate scheduling\n",
    "5. âœ… Grad-CAM visualization for interpretability\n",
    "6. âœ… Complete training and inference pipeline\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download Data**: Get the Kaggle Dogs vs Cats dataset\n",
    "2. **Train Model**: Use `main_train.py` for full training\n",
    "3. **Evaluate**: Analyze performance on test set\n",
    "4. **Deploy**: Launch web interface with `web_interface.py`\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "With proper training:\n",
    "- **Accuracy**: 98-99%\n",
    "- **AUC**: 0.99+\n",
    "- **Training Time**: 2-4 hours on modern GPU\n",
    "\n",
    "### References:\n",
    "\n",
    "- Kaggle Dogs vs Cats: https://www.kaggle.com/c/dogs-vs-cats\n",
    "- Grad-CAM Paper: https://arxiv.org/abs/1610.02391\n",
    "- Mixup Paper: https://arxiv.org/abs/1710.09412\n",
    "- CutMix Paper: https://arxiv.org/abs/1905.04899"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
